# ðŸ“§ Spam Email Classification using Naive Bayes & TF-IDF

This project implements a **Spam Email Classifier** using **Multinomial Naive Bayes** and **TF-IDF vectorization**.  
It classifies messages as either **Spam** or **Ham (Not Spam)**.

---

## ðŸ“Œ Project Overview
- Dataset: `spam_sample.csv` containing email/text messages and their labels.
- Preprocessing:
  - Converted text to lowercase.
  - Tokenized text using NLTK.
  - Removed stopwords and punctuation.
  - Applied stemming using **Porter Stemmer**.
- Feature Extraction: Used **TF-IDF (Term Frequency - Inverse Document Frequency)**.
- Model: **Multinomial Naive Bayes** for classification.
- Evaluation Metrics:
  - Confusion Matrix
  - Classification Report (Precision, Recall, F1-score)
  - Accuracy Score

---

## ðŸ“‚ Dataset
The dataset contains:
- `message` â†’ The email/text message.  
- `label` â†’ The target (spam or ham).  

Example:
```
label    message
spam     Congratulations! Youâ€™ve won!
ham      Are we meeting tomorrow?
```

---

## ðŸš€ Installation & Setup
 Download NLTK resources (first time only):
   ```python
   import nltk
   nltk.download('punkt')
   nltk.download('stopwords')
   ```



---

## ðŸ§¹ Data Preprocessing

Steps applied in preprocessing:
- Convert text to lowercase  
- Tokenization (`word_tokenize`)  
- Remove stopwords and punctuation  
- Apply stemming with Porter Stemmer  


---

## ðŸ“Š Model Training

- Feature extraction using TF-IDF:
```python
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['clean_text'])
```

- Train-Test Split (80/20).  
- Train model using **Multinomial Naive Bayes**.  
- Evaluate with accuracy, confusion matrix, and classification report.

---

## âœ… Results

Example Output:
```
Accuracy: 0.97
Confusion Matrix:
[[965   3]
 [ 12 150]]

Classification Report:
              precision    recall  f1-score   support
         ham       0.99      0.99      0.99       968
        spam      0.98      0.93      0.96       162
```
